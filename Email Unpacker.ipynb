{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "def read_gmail():\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.modify']\n",
    "    unread_messages = []\n",
    "    creds = None\n",
    "    \n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "    # Call the Gmail API\n",
    "    inbox_unreads = service.users().messages().list(userId='me',labelIds = ['INBOX', 'UNREAD']).execute()\n",
    "    \n",
    "    if inbox_unreads['resultSizeEstimate'] != 0:\n",
    "        for unread in inbox_unreads['messages']:\n",
    "            unread_messages.append(service.users().messages().get(userId='me', id=unread['id']).execute())\n",
    "            service.users().messages().modify(userId='me', id=unread['id'], body={\"removeLabelIds\": [\"UNREAD\"]}).execute()\n",
    "\n",
    "    return unread_messages\n",
    "\n",
    "new_gmail_messages = read_gmail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import base64\n",
    "import re\n",
    "\n",
    "def purify_email_address(str_with_email):\n",
    "    \"\"\"\n",
    "    Given string containing an email address potentially surrounded by some characters\n",
    "    Returns a string that is only the email address\n",
    "    \"\"\"\n",
    "    email_regex = r'[\\w\\.-]+@[\\w\\.-]+'\n",
    "    return re.findall(email_regex, str_with_email)[0]\n",
    "\n",
    "def separate_url_text(str_with_urls):\n",
    "    \"\"\"\n",
    "    Given string containing any number of urls e.g. https://google.com or http://site.com?q=123\n",
    "    Returns a set of unique urls found and the original string free of urls \n",
    "    \"\"\"\n",
    "    url_regex = r'(https?:\\/\\/(?:(?:[^\\s()<>]))+)'\n",
    "    urls = set(re.findall(url_regex, str_with_urls))\n",
    "    url_free_text = re.sub(url_regex, '', str_with_urls)\n",
    "    return urls, url_free_text\n",
    "\n",
    "def remove_special_characters(str_with_special_chars):\n",
    "    \"\"\"\n",
    "    Given a string containing any number of special characters\n",
    "    Returns a string free of special characters\n",
    "    \"\"\"\n",
    "    special_char_regex = r'[><+\\|_\\-=*#$\\)(\\?\\&:.,!@%\\^}{]'\n",
    "    str_with_special_chars = str_with_special_chars.replace('\\W', '')\n",
    "    return re.sub(special_char_regex, '', str_with_special_chars)\n",
    "\n",
    "def clean_stem_words(words):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words if word not in set(stop_words)]\n",
    "\n",
    "def parse_gmail_headers_array(headers_array, msg_data):\n",
    "    # Collapses headers given as \n",
    "    # [{name: \"stuff\", value: \"value\"}, {name: \"other\", value: \"other\"}] -> {stuff: \"value\", other: \"other\"}\n",
    "    # and adds header info to the msg_data struct\n",
    "    for header in headers_array:\n",
    "        if header['name'] == 'Subject':\n",
    "            msg_data['subject'] = header['value'].lower()\n",
    "            msg_data['subject-clean'] = \" \".join(clean_stem_words(\n",
    "                remove_special_characters(msg_data['subject']).split()\n",
    "            ))\n",
    "        if header['name'] == 'From':\n",
    "            frm = purify_email_address(header['value'])\n",
    "            msg_data['participants'].append(frm)\n",
    "            msg_data['participants-count'] += 1\n",
    "        if header['name'] == \"Cc\":\n",
    "            for participant in header['value'].split(\", \"):\n",
    "                participant_emails = re.findall(\"[\\w\\.-]+@[\\w\\.-]+\", participant)\n",
    "                if len(participant_emails) > 0:\n",
    "                    msg_data['participants'].append(participant_emails[0])\n",
    "            msg_data['participants-count'] += len(msg_data['participants'])\n",
    "        if header['name'] == \"References\" :\n",
    "            msg_data[\"thread-length\"] = (len(header['value'].split(\" \")) + 1)\n",
    "        if header['name'] == \"Date\" :\n",
    "            msg_data[\"time-rec\"] = parser.parse(header['value']).timestamp()\n",
    "\n",
    "def parse_gmail_body(gmail_body_parts, msg_data):\n",
    "    for part in gmail_body_parts:\n",
    "        if part['body'].get('data'):\n",
    "            decodedBytes = base64.urlsafe_b64decode(part['body']['data'])\n",
    "            decodedStr = str(decodedBytes, \"utf-8\").lower()\n",
    "            \n",
    "            if part['mimeType'] == \"text/html\":\n",
    "                pass\n",
    "            elif part['mimeType'] == \"text/plain\":\n",
    "                msg_data['urls'], decodedStr = separate_url_text(decodedStr)\n",
    "                decodedStr = remove_special_characters(decodedStr)\n",
    "                msg_data['words'] = \" \".join(clean_stem_words(decodedStr.split()))\n",
    "            else:\n",
    "                print(\"unrecognized mimeType\")\n",
    "        else:\n",
    "            if part['body'].get('attachmentId'):\n",
    "                msg_data['attachments'] += 1\n",
    "            if part.get('parts'):\n",
    "                parse_gmail_body(part['parts'], msg_data)\n",
    "                \n",
    "def unpack_message(msg):\n",
    "    msg_data = {\n",
    "        \"id\": msg['id'],\n",
    "        \"subject\": \"\",\n",
    "        \"subject-clean\": \"\",\n",
    "        \"words\": \"\",\n",
    "        \"time-proc\": datetime.now().timestamp(),\n",
    "        \"attachments\": 0,\n",
    "        \"thread-length\": 0,\n",
    "        \"participants-count\": 0,\n",
    "        \"urls\": [],\n",
    "        \"participants\": []\n",
    "    }\n",
    "\n",
    "    parse_gmail_headers_array(msg['payload']['headers'], msg_data)\n",
    "    if msg['payload'].get('parts'):\n",
    "        parse_gmail_body(msg['payload']['parts'], msg_data)\n",
    "    \n",
    "    return msg_data\n",
    "\n",
    "unpacked_messages = []\n",
    "for msg in new_gmail_messages:\n",
    "    unpacked_messages.append(unpack_message(msg))\n",
    "    \n",
    "action_map = {\"ignore\": 0, \"read\": 1, \"respond\": 2}\n",
    "\n",
    "def prototype_dataframe(messages):\n",
    "    columns = [k for k in messages[0].keys() if k not in [\n",
    "        \"id\", \n",
    "        \"subject\", \n",
    "        \"words\", \n",
    "        \"participants\", \n",
    "        \"subject-clean\", \n",
    "        \"time-rec\", \n",
    "        \"time-proc\",\n",
    "        \"urls\"\n",
    "    ]]\n",
    "    index = [msg[\"id\"] for msg in messages]\n",
    "    \n",
    "    for msg in messages:\n",
    "        msg['text'] = msg['subject-clean'] + \" \" + msg['words'] + \" \" + \" \".join(msg['participants'])\n",
    "        msg['link-count'] = len(msg['urls'])\n",
    "        # Generate some random guesses for now... remove later\n",
    "        if msg['thread-length'] > 1:\n",
    "            msg['label'] = random.randint(1, 2)\n",
    "        elif msg['attachments'] > 0:\n",
    "            msg['label'] = action_map[\"respond\"]\n",
    "        else:\n",
    "            msg['label'] = random.randint(0, 2)\n",
    "\n",
    "    return pd.DataFrame(messages, columns=columns + [\"link-count\", \"text\", \"label\"], index=index)\n",
    "\n",
    "df = prototype_dataframe(unpacked_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_ngrams = tfidf_vect.fit_transform(df['text'])\n",
    "train_messages, test_messages, train_labels, test_labels = train_test_split(X_ngrams, df['label'], test_size=0.3, random_state=42, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "%time mnb.fit(train_messages, train_labels)\n",
    "mnbpred = mnb.predict(test_messages)\n",
    "\n",
    "print('Multinomial Naive Bayes F1 Score :', metrics.f1_score(test_labels, mnbpred, average='weighted'))\n",
    "# cross-validation using confusion matrix\n",
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(test_labels, mnbpred),\n",
    "    index=[['actual', 'actual', 'actual'], list(action_map.keys())], \n",
    "    columns=[['predicted', 'predicted', 'predicted'], list(action_map.keys())]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(mnb)\n",
    "%time abc.fit(train_messages, train_labels)\n",
    "abcpred = abc.predict(test_messages)\n",
    "\n",
    "print('AdaBossted Naive Bayes F1 Score :', metrics.f1_score(test_labels, abcpred, average='weighted'))\n",
    "# cross-validation using confusion matrix\n",
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(test_labels, abcpred),\n",
    "    index=[['actual', 'actual', 'actual'], list(action_map.keys())], \n",
    "    columns=[['predicted', 'predicted', 'predicted'], list(action_map.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, max_depth=4, bootstrap=False)\n",
    "%time rf.fit(train_messages, train_labels)\n",
    "rfpred = rf.predict(test_messages)\n",
    "\n",
    "print('Random Forest F1 Score :', metrics.f1_score(test_labels, rfpred, average='weighted'))\n",
    "# cross-validation using confusion matrix\n",
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(test_labels, rfpred),\n",
    "    index=[['actual', 'actual', 'actual'], list(action_map.keys())], \n",
    "    columns=[['predicted', 'predicted', 'predicted'], list(action_map.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcrf = AdaBoostClassifier(rf)\n",
    "%time abcrf.fit(train_messages, train_labels)\n",
    "abcpred = abcrf.predict(test_messages)\n",
    "\n",
    "print('AdaBossted Random Forest F1 Score :', metrics.f1_score(test_labels, abcpred, average='weighted'))\n",
    "# cross-validation using confusion matrix\n",
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(test_labels, abcpred),\n",
    "    index=[['actual', 'actual', 'actual'], list(action_map.keys())], \n",
    "    columns=[['predicted', 'predicted', 'predicted'], list(action_map.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "   \n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, refit = True, verbose = 3, n_jobs=6) \n",
    "%time grid.fit(train_messages, train_labels)\n",
    "pgpred = grid.predict(test_messages)\n",
    "print('SVM F1 Score :', metrics.f1_score(test_labels, pgpred, average='weighted'))\n",
    "print(grid.best_params_)\n",
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(test_labels, pgpred),\n",
    "    index=[['actual', 'actual', 'actual'], list(action_map.keys())], \n",
    "    columns=[['predicted', 'predicted', 'predicted'], list(action_map.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = SVC()\n",
    "%time svc.fit(train_messages, train_labels)\n",
    "svcpred = svc.predict(test_messages)\n",
    "\n",
    "print('SVM F1 Score :', metrics.f1_score(test_labels, svcpred, average='weighted'))\n",
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(test_labels, svcpred),\n",
    "    index=[['actual', 'actual', 'actual'], list(action_map.keys())], \n",
    "    columns=[['predicted', 'predicted', 'predicted'], list(action_map.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'gamma':['scale', 'auto'],\n",
    "              'kernel': ['linear']}  \n",
    "   \n",
    "grid = GridSearchCV(SVC(random_state=42), param_grid=param_grid, cv= 5) \n",
    "%time grid.fit(train_messages, train_labels)\n",
    "svcgpred = grid.predict(test_messages)\n",
    "print('SVM F1 Score :', metrics.f1_score(test_labels, svcgpred, average='weighted'))\n",
    "print(grid.best_params_)\n",
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(test_labels, svcgpred),\n",
    "    index=[['actual', 'actual', 'actual'], list(action_map.keys())], \n",
    "    columns=[['predicted', 'predicted', 'predicted'], list(action_map.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "action_map = {\"ignore\": 0, \"read\": 1, \"respond\": 2}\n",
    "\n",
    "def new_data_dataframe(messages):\n",
    "    columns = [k for k in messages[0].keys() if k not in [\n",
    "        \"id\", \n",
    "        \"words\", \n",
    "        \"participants\", \n",
    "        \"subject-clean\", \n",
    "        \"time-rec\", \n",
    "        \"time-proc\",\n",
    "        \"urls\"\n",
    "    ]]\n",
    "    index = [msg[\"id\"] for msg in messages]\n",
    "    \n",
    "    for msg in messages:\n",
    "        msg['text'] = msg['subject-clean'] + \" \" + msg['words'] + \" \" + \" \".join(msg['participants'])\n",
    "        msg['link-count'] = len(msg['urls'])\n",
    "        msg['label'] = -1\n",
    "\n",
    "    return pd.DataFrame(messages, columns=columns + [\"link-count\", \"text\", \"label\"], index=index)\n",
    "\n",
    "ngms = read_gmail()\n",
    "ums = []\n",
    "for msg in ngms:\n",
    "    ums.append(unpack_message(msg))\n",
    "    \n",
    "udf = new_data_dataframe(ums)\n",
    "new_preds = mnb.predict(tfidf_vect.transform(udf['text']))\n",
    "i = 0\n",
    "for c, n in udf.iterrows():\n",
    "    udf.loc[c, 'label'] = new_preds[i]\n",
    "    i+=1\n",
    "udf\n",
    "# write the new labels back to the model (async process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_excel('../web_api/emails.xlsx', engine='openpyxl').describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}